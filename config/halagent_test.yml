epoch: 10
steps_per_epoch: 200

SAC:
  gamma: 0.99
  polyak: 0.005
  lr: 0.001
  alpha: 0.2

policy:
  n_scan: 1
  n_vo_history: 10
  sensor_horizon: 8.0
  plan_interval: 0.5
  act_dim: 4
  # act_dim: 5
  combine_scans: True
  policy_hz: 1.0

episode:
  timeout: 60.0
  opponents:
    - vanilla
    - baseline
    - phhp

train:
  explore_steps: 300
  update_after: 100
  replay_size: 3000
  batch_size: 64

reward:
  C_PLAN_CHANGE: 5.0
  C_STOP: 1.0
  C_SUCCESS: 30.0

save_freqency: 100