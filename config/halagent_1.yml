epoch: 200
steps_per_epoch: 2000

SAC:
  gamma: 0.99
  polyak: 0.005
  lr: 0.001
  alpha: 0.2

policy:
  n_scan: 10
  n_vo_history: 10
  sensor_horizon: 8.0
  plan_interval: 0.5
  act_dim: 4
  # act_dim: 5
  combine_scans: True
  policy_hz: 1.0

episode:
  timeout: 60.0
  opponents:
    - vanilla
    # - baseline
    # - phhp

train:
  explore_steps: 3000
  update_after: 1000
  replay_size: 30000
  batch_size: 1024

reward:
  C_PLAN_CHANGE: 5.0
  C_STOP: 2.0
  C_SUCCESS: 100.0

n_test: 1
save_freqency: 1